# -*- coding: utf-8 -*-
"""Classification_of_KNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pQsn7Fl2XVaxJftNZGxia6epSnPHZsvd

## Objectives

After completing this lab you are able to:

 - Understand the principles of the KNN algorithm.
 - Perform data preprocessing techniques such as scaling and normalization to prepare data for effective KNN modeling.
 - Select the optimal number of neighbors for the KNN algorithm by using methods like cross-validation to enhance the model's prediction accuracy.
 - Evaluate the performance of your KNN model by using metrics such as accuracy and confusion matrices.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# %pip install numpy==1.26.4 pandas==2.2.2 matplotlib==3.8.4 seaborn==0.13.2 scikit-learn==1.5.0 xlrd==2.0.1 openpyxl==3.1.4 --force-reinstall

from tqdm import tqdm
import numpy as np
import pandas as pd
from itertools import accumulate
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import scale, LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.metrics import accuracy_score, cohen_kappa_score, confusion_matrix
from sklearn.feature_selection import f_classif
from sklearn.utils import resample

# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

sns.set_context('notebook')
sns.set_style('white')

df = pd.read_excel('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/X4i8vXLw81g4wEH473zIFA/Diabetes-Classification.xlsx')

df.head()

df.drop(columns=['Unnamed: 16', 'Unnamed: 17'])

frequency_table = df['Diabetes'].value_counts()
props = frequency_table.apply(lambda x: x / len(df['Diabetes']))
print(props)

"""This results show that the dataset is imbalanced, with 84.6% non-diabetic cases and only 15.4% diabetic cases. This imbalance can bias classification models toward the majority class, making it necessary to apply techniques to handle class imbalance."""

df_reduced = df[["Diabetes", "Cholesterol", "Glucose", "BMI", "Waist/hip ratio",
                 "HDL Chol", "Chol/HDL ratio", "Systolic BP", "Diastolic BP",
                 "Weight"]]

numerical_columns = df_reduced.iloc[:, 1:10]

# Applying scaling
scaler = StandardScaler()
preproc_reduced = scaler.fit(numerical_columns)

df_standardized = preproc_reduced.transform(numerical_columns)

# Converting the standardized array back to DataFrame
df_standardized = pd.DataFrame(df_standardized, columns=numerical_columns.columns)

df_standardized.describe()

df_stdize = pd.concat([df_reduced['Diabetes'], df_standardized], axis=1)
df_stdize

X = df_stdize.drop(columns=['Diabetes'])
y = df_stdize['Diabetes']

# Split the data into training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,
                                                    random_state=42)

"""The predictor variables (X) and target variable (y) were separated, and the dataset was split into training (80%) and testing (20%) sets. This split allows the model to be trained on one portion of the data and evaluated on unseen data, ensuring an unbiased assessment of model performance."""

# Initialize LabelEncoder
label_encoder = LabelEncoder()

y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.fit_transform(y_test)

"""The categorical target variable (Diabetes) was converted into numerical labels using LabelEncoder,
allowing the model to process the class labels in numeric form for training and testing.

## Fit the KNN model
"""

# Create a KNN classifier
knn = KNeighborsClassifier()

knn.fit(X_train, y_train_encoded)

#calculate overall accuracy
y_pred = knn.predict(X_test)
accuracy = accuracy_score(y_test_encoded, y_pred)
print(f'Accuracy: {accuracy:.2%}')

"""The KNN classifier was trained on the training data and evaluated on the test set, achieving an accuracy of 88.46%. This indicates that the model correctly classified approximately 88 out of every 100 observations, showing good predictive performance in distinguishing between diabetic and non-diabetic cases.

## Hyperparameter tuning
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV # Ensure GridSearchCV is also imported if not universally available in the notebook

# Create a KNN classifier
knn = KNeighborsClassifier()

param_grid = {'n_neighbors': range(1, 12)}

# Perform grid search with cross-validation
grid_search = GridSearchCV(knn, param_grid, cv=10)
grid_search.fit(X_train, y_train_encoded)


# Best parameters and best score
print("Best parameters found: ", grid_search.best_params_)
print(f"Best accuracy score: , {grid_search.best_score_:.3f}")

# Full results
results = grid_search.cv_results_
for mean_score, std_score, params in zip(results['mean_test_score'], results['std_test_score'], results['params']):
    print(f"Mean accuracy: {mean_score:.3f} (std: {std_score:.3f}) with: {params}")

"""Hyperparameter tuning using 10-fold cross-validation was performed to identify the optimal number of neighbors (k) for the KNN model. The best performance was achieved with k = 7, yielding a cross-validated accuracy of 91.7%. The results show that model accuracy improves as k increases up to this point and then stabilizes, indicating that moderate values of k provide better generalization and reduce overfitting compared to very small values.

## ANOVA for feature selection
"""

fs_score, fs_p_value = f_classif(X, y)

# Combine scores with feature names
fs_scores = pd.DataFrame({'Feature': X.columns, 'F-Score': fs_score, 'P-Value': fs_p_value})
fs_scores = fs_scores.sort_values(by='F-Score', ascending=False)

print(fs_scores)

from sklearn.feature_selection import f_classif
import pandas as pd
import matplotlib.pyplot as plt

# ANOVA F-test
X_balanced = balanced.drop(columns=['Diabetes'])
y_balanced = balanced['Diabetes']
fs_score, fs_p_value = f_classif(X_balanced, y_balanced)

fs_scores = pd.DataFrame({
    'Feature': X_balanced.columns,
    'F-Score': fs_score
}).sort_values(by='F-Score', ascending=False)

# Plot
plt.figure(figsize=(10,6))
plt.barh(fs_scores['Feature'], fs_scores['F-Score'])
plt.gca().invert_yaxis()
plt.xlabel("F-Score")
plt.title("Feature Importance (ANOVA F-Test)")
plt.show()

# Save image
plt.savefig("images/feature_importance.png", dpi=300, bbox_inches='tight')

"""ANOVA (F-test) feature selection results show that Glucose is the most significant predictor of diabetes, with a very high F-score and a near-zero p-value, indicating a strong statistical association with the outcome. Variables such as Chol/HDL ratio, Cholesterol, and Systolic BP also show significant relationships with diabetes. In contrast, Diastolic BP has a low F-score and a non-significant p-value, suggesting it contributes little to diabetes classification in this dataset.

#Downsampling
"""

# Converting Diabetes column into binary (0 for No Diabetes and 1 for Diabetes)
df_stdize['Diabetes'] = np.where(df_stdize['Diabetes'] == 'Diabetes', 1, 0)
df_stdize

"""The Diabetes variable was converted into a binary numerical format, where 1 represents diabetes and 0 represents no diabetes, making the target variable suitable for binary classification and further modeling."""

# Number of rows for positive diabetes
positive_diabetes = df_stdize[df_stdize['Diabetes'] == 1].shape[0]
print('Number of rows for positive diabetes: ', positive_diabetes)

# Sample negative cases to match positive cases
negative_diabetes = df_stdize[df_stdize['Diabetes'] == 0]
negative_diabetes_downsampled = resample(negative_diabetes, replace=False, n_samples=positive_diabetes, random_state=42)

# Put positive and negative diabetes case into one df -> balanced
balanced = pd.concat([negative_diabetes_downsampled, df_stdize[df_stdize['Diabetes'] == 1]])
balanced.sample(5)

"""The number of diabetic cases was first identified, and the non-diabetic class was then downsampled to match this count. Both classes were combined to create a balanced dataset, ensuring equal representation of diabetic and non-diabetic cases and reducing bias during model training."""

balanced['Diabetes'].value_counts()

"""The balanced dataset contains an equal number of observations for both classes, with 60 non-diabetic (0) and 60 diabetic (1) cases, confirming that class imbalance has been successfully addressed.

## Fitting on simpler model
"""

X_simple = balanced[['Glucose']]
y = balanced['Diabetes']

# Split the data
X_train_simple, X_test_simple, y_train_simple, y_test_simple = train_test_split(X_simple,
                                                                                y, test_size=0.2,
                                                                                random_state=42)

knn_simple = KNeighborsClassifier()
knn_simple.fit(X_train_simple, y_train_simple)
y_pred_simple = knn_simple.predict(X_test_simple)
accuracy = accuracy_score(y_test_simple, y_pred_simple)
print(f'Accuracy: {accuracy:.2%}')

"""The simplified KNN model trained using only glucose levels achieved an accuracy of 91.67%, indicating that glucose alone is a highly effective predictor of diabetes in the balanced dataset.

## Evaluating KNN
"""

# Evaluate confusion matrix
cm = confusion_matrix(y_test_encoded, y_pred)

# Print confusion matrix
print("Confusion Matrix:")
print(cm)
accuracy = accuracy_score(y_test_encoded, y_pred)
print(f'Accuracy: {accuracy:.2%}')

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import os

# Predictions
y_pred = grid_search.best_estimator_.predict(X_test)

# Confusion matrix
cm = confusion_matrix(y_test_encoded, y_pred)

# Plot
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=["Non-Diabetic", "Diabetic"])

disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix - KNN Diabetes Classification")
plt.show()

# Create 'images' directory if it doesn't exist
output_dir = 'images'
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

# Save image
plt.savefig(os.path.join(output_dir, "confusion_matrix.png"), dpi=300, bbox_inches='tight')

"""The confusion matrix shows that the model correctly classified 7 non-diabetic and 61 diabetic cases, while misclassifying 9 non-diabetic cases as diabetic and 1 diabetic case as non-diabetic. The overall accuracy of 88.46% indicates good model performance, with slightly better detection of diabetic cases than non-diabetic cases."""

from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier
import matplotlib.pyplot as plt

k_values = range(1, 12)
accuracies = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_train, y_train_encoded, cv=10, scoring='accuracy')
    accuracies.append(scores.mean())

# Plot
plt.figure(figsize=(8,5))
plt.plot(k_values, accuracies, marker='o')
plt.xlabel("Number of Neighbors (k)")
plt.ylabel("Accuracy")
plt.title("Model Accuracy vs Number of Neighbors (k)")
plt.xticks(k_values)
plt.grid(True)
plt.show()

# Save image
plt.savefig("images/model_accuracy_vs_k.png", dpi=300, bbox_inches='tight')